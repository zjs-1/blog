---
title: '1 -《Supervised Machine Learning: Regression and Classification》'
date: 2022-10-21 11:30:14
tags: [学习笔记,机器学习]
categories: 技术
toc: true
---

## 机器学习

定义：使计算机可以无需明确编程就可以学习的研究领域。

机器学习的两种主要类型为监督学习(Supervised learning)和无监督学习(Unsupervised learning)

### 监督学习

监督学习是指学习x到y或者输入到输出的映射。典型的例子有:

- 垃圾邮件的判断，输入x是一封邮件，输出y就是这封邮件是否为垃圾邮件
- 房价的预测，输入x是房子的相关信息，输出y就是房子的价格

监督学习算法有两种主要类型，分别是回归算法(regression algorithms)和分类算法(classification algorithm)。
简单说两者的区别，回归算法用于从无限多的数字内预测一个数字，而分类算法则从一小组可能的输出结果中，预测一个分类。
基于这个区别，垃圾邮件的判断适用分类算法，房价的预测适用回归算法。

### 无监督学习

无监督学习与监督学习的区别是: 监督学习的训练数据集中，每个输入都有对应的输出，比如每一封邮件都会有对应的判断，是否为垃圾邮件。而无监督学习的训练数据集则不会包含输出。

我个人的理解: 监督学习算法是，我们提供一堆邮件，并且给它们人为标上是否为垃圾邮件，让计算机根据这些人为判断的结果，学习判断哪些是垃圾邮件。而无监督学习，是直接提供一堆邮件，让计算机学习这些邮件之间可能存在哪些分类方式。

典型的无监督学习的例子，新闻网站可以将所有的新闻文章作为输入，通过无监督学习算法，可以实现相关新闻推荐。
这种无监督学习的算法称为聚类算法，此外还有异常检测和降维(dimensionality reduction)两种常见算法。

## 线性回归

> 线性回归应该是最简单的模型了，通过学习训练线性回归模型，有利于理解很多模型的概念。

线性回归模型，用于将一条直线拟合到训练数据中，通过这条直线可以预测数字，是监督学习模型的一种。

![](http://cdn.jsblog.site/16656298857725.jpg)
如上图，每一个`x`点都是训练数据，找到最贴近这些所有数据的直线，用其来预测新的数据，这就是一个线性回归模型。

### 一些名词

训练集(training set): 用于训练模型的数据，比如我们已经掌握的房价数据

输入变量 或 特征(feature) 或 输入特征: 一般用`x`表示，比如一栋房子的尺寸是100平，那么一个输入特征就是100。有些模型可能多个输入特征，比如房子的尺寸、房子的年份，则会用{% mathjax %}x_{1}$、$\\x_{2}{% endmathjax %}等来表示

输出变量 或 目标变量: 一般用`y`表示，在预测房价的例子里，输出变量就是预测的房价

训练示例(training example): 训练集里一个数据，常用`(x, y)`表示。

除了上面的x、y表示，还有一些常用的代数表示：
m: 训练集的大小，代表有多少个训练示例
n: 特征的数量，这个模型有两个训练特征，则n=2
{% mathjax %}x^{(i)}_{(j)}{% endmathjax %}: 第i个训练示例的第j个输入特征
{% mathjax %}y^{(i)}{% endmathjax %}: 第i个训练示例的输出
{% mathjax %}\hat{y}{% endmathjax %}: 模型预测的输出值，是对y的预测
{% mathjax %}f_{w,b}(x){% endmathjax %}: 模型的表示，输入为x，w和b是模型的参数，模型训练的目标就是得到合适的参数

### 单元线性回归

> 线性回归是比较简单的监督学习模型，而其中单元线性回归，又是其中最简化的一种，它指的是只有一个输入特征的线性回归模型。

#### 模型

从上面的图可以看出来，房价的预测模型可能是一条直线，以过去的数学经验可以知道，直线的表达式为{% mathjax %}y = ax + b$。所以在房价预测这个应用里，我们可以选择$\\f_{w,b}(x) = wx + b{% endmathjax %}这个模型，通过训练模型找到合适的参数w和b，使得这条直线可以尽可能贴合训练数据。

#### 成本函数

衡量模型是否贴合数据，需要通过成本函数(Cost function)来量化计算。

成本函数一般用`J`进行表示，可写作J(w,b)。对于线性回归模型，一般会选用`平方误差成本函数(squared error cost function)`作为成本函数，表达式为{% mathjax %}J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2{% endmathjax %}。

![](http://cdn.jsblog.site/16656339242755.jpg)

> 表达式看起来有点复杂，通过上图可以比较好的理解

图中蓝线为我们的预测模型，红色的`x`是训练数据。则每一个`x`到蓝线的距离就可以衡量模型的误差(error)，写作{% mathjax %}\hat{y}^{i}-y^{i} = f_{w,b}(x^{(i)}) - y^{(i)}{% endmathjax %}。
因为预测值可能比实际值低，所以{% mathjax %}\hat{y}^{i}-y^{i}{% endmathjax %}可能为负数，一般来说需要取绝对其进行处理。但是绝对数不利于后续的计算，所以这里会对其进行乘方操作来消除负数。
再将所有训练示例的数据都计算一遍，进行累加，再除以`m`，就可以得到我们需要的成本函数了。不过成本函数里，实际是除以了`2m`，目的同样是利于后续的计算。其实是否除2，对成本函数的作用本身，无任何影响。


#### 成本函数可视化

> {% mathjax %}J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(w*x^{(i)}+b - y^{(i)})^2{% endmathjax %}

我们首先假设b=0，则成本函数变成{% mathjax %}J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(w*x^{(i)} - y^{(i)})^2$，需要注意的是，其中的$\\x^{(i)}$和$y^{(i)}{% endmathjax %}在某次训练中，其实是已知固定变量，可以当成常量看待。

![](http://cdn.jsblog.site/16656365032587.jpg)
将成本函数绘制出来，可能会是如上图一条抛物线，当`w`为某个值时，J(w)的值达到最低点。

当b不等于0时，绘制出来的图形将会是一个三维图形，如下图
![](http://cdn.jsblog.site/16656404288770.jpg)

看起来就像是一个碗，当w和b为某个值时，J(w,b)会达到最低点

![](http://cdn.jsblog.site/16656405081157.jpg)

上图为另一种表示方式，其中水平轴为w，垂直轴为b，每一个椭圆线上的所有点，对应的J(w,b)值都相等，最里面的椭圆的中心点处，J(w,b)达到最小值。这种绘制方式有点像等高线地形图。

#### 梯度下降(gradient descent)

> 通过观察可视化后的成本函数，我们可以通过肉眼快速地找到最合适的参数w和b，使得成本函数达到最低，即使得模型最贴合数据。但是一些更为复杂的模型，我们很难通过肉眼找到最合适的参数，需要一些方法让计算机可以自己调整参数。梯度下降就是方法之一，它也是机器学习中很重要的一部分。

![](http://cdn.jsblog.site/16656410814666.jpg)

通过上面这个图，可以简单地对梯度下降的原理进行理解。图是成本函数的三维图，图里红色代表J比较大，蓝色代表J比较小，有点像是此起彼伏的山坡。想象选定了一个位置，从这个位置环顾四周，选一个往下走坡度最大的方向，向这个方向走一步。然后再重复这个动作，直至四周已经没有更低的地方，说明此处的坐标(w,b)可以使得成本函数J处于一个较小的值，称为局部最小值(local minima)。

假如换了一个初始位置，重新进行梯度下降，我们可能得到另一个局部最小值。

#### 梯度下降的实现

> 可视化仅仅是便于理解，需要通过标准化的实现才能让程序自行训练
> 梯度下降的实现，便是通过一定规则的计算，调整参数w和b

其计算公式是
{% mathjax %}w = w - \alpha\frac{d}{dw}J(w,b)$ 和 $\\b = b - \alpha\frac{d}{db}J(w,b){% endmathjax %}

有几点需要注意：
1. {% mathjax %}\alpha{% endmathjax %}为学习率(learning rate)，在实际操作中需要根据一定的经验进行调整，学习率过大，会导致无法正确到达局部最小值，学习率过小，则会导致学习过程太长，效率太低。
2. 对w和b的更新需要保证同步，不能先更新w，然后再用更新后的w计算b，必须先将{% mathjax %}\alpha\frac{d}{dw}J(w,b)$和$\\\alpha\frac{d}{db}J(w,b){% endmathjax %}计算好，然后同步更新。
3. {% mathjax %}\frac{d}{dw}J(w,b){% endmathjax %}即对函数J，对变量w进行求导

![](http://cdn.jsblog.site/16656425274957.jpg)

> 高数的知识已经有点模糊，已经说不出求导具体的定义是什么

我的理解是，导数可以表明数据的变化趋势。比如上图里，右边的点，求导的值为2，大于0，表明其为上升趋势，若再往右一点，其求导值可能为3，代表其上升趋势更大。左边的点求导的值小于0，所以为下降趋势。中间位置求导的值为0，说明在此处趋势将从下降变为上升，即处于最低点。

**在单元线性回归里的实现**
将前文的J(w,b)代入上面的公式，可以得到
{% mathjax %}w = w - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}{% endmathjax %}
{% mathjax %}b = b - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)}){% endmathjax %}

上面是求导后得到的结果，可以直接使用，不过实在是不服输，还是用所剩不多的数学知识，推导了下过程:
{% mathjax %}\frac{d}{dw}(\frac{1}{2m}\sum_{i=1}^{m}(wx^{(i)}+b - y^{(i)})^2){% endmathjax %}
{% mathjax %}\frac{d}{dw}(\frac{1}{2m}\sum_{i=1}^{m}(w^2x^{(i)2} + 2wx^{(i)}(b - y^{(i)}) + (b - y^{(i)})^2)){% endmathjax %}
{% mathjax %}\frac{1}{2m}\sum_{i=1}^{m}(2wx^{(i)2} + 2x^{(i)}(b - y^{(i)})){% endmathjax %}
{% mathjax %}\frac{1}{2m}\sum_{i=1}^{m}(wx^{(i)} + b - y^{(i)})2x^{(i)}{% endmathjax %}
{% mathjax %}\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}{% endmathjax %}

### 多元线性回归

> 前面的部分，我们都只考虑只有一个输入特征的线性回归，然而现实中更常见的是会有多个变量同时影响一个结果，也就是有多个输入特征。

#### 模型

多元线性回归的模型方程可以写作:{% mathjax %}f_{w,b}(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b{% endmathjax %}
简写为: {% mathjax %}f_{\overrightarrow{W},b}(\overrightarrow{X}) = \overrightarrow{W} \bullet \overrightarrow{X} + b{% endmathjax %}

其中需要解释的：
1. x1到xn是多个输入特征，而w1到wn和b是模型的参数
2. {% mathjax %}\overrightarrow{W}$是一个包含了w1到wn的数字列表，称为向量(vector)。$\\\overrightarrow{X}{% endmathjax %}同理
3. {% mathjax %}\bullet{% endmathjax %} 是向量的`点积`运算，具体的定义可以参见百度。**需要特别注意的是，在代码编程中，使用矢量化的运算可以比较好的利用计算机的并行能力，比如python中NumPy的dot函数，可以高效地计算`点积`，比起普通的循环计算效率高很多。**

#### 成本函数
回忆下单元线性回归的成本函数：{% mathjax %}J(w,b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})^2{% endmathjax %}
可以看出，多元线性回归的成本函数，是只有{% mathjax %}f_{w,b}(x^{(i)}{% endmathjax %}只有这部分的计算结果有变化，成本的计算过程是完全一致的。

#### 梯度下降实现
同样回忆下单元线性回归的梯度下降实现：
{% mathjax %}w = w - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}{% endmathjax %}
{% mathjax %}b = b - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)}){% endmathjax %}

可以注意到，w的更新过程中，需要乘一个{% mathjax %}x^{(i)}{% endmathjax %}，但是在多元线性回归中，会有多个输入特征，那这里应该乘以哪一个输入特征呢？
这一点就是多元线性回归和单元线性回归，在梯度下降实现上的差异点，先直接上结论：
{% mathjax %}w_{1} = w_{1} - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_1{% endmathjax %}
{% mathjax %}w_{n} = w_{n} - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_n{% endmathjax %}
{% mathjax %}b = b - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)}){% endmathjax %}

推导过程跟单元线性回归的很类似，因为写公式有点麻烦，这里就略过了。只要记住一点，求导的时候，除了被求导的那个变量，其他任何代数符号，都是常量。比如对w1进行求导，则w2、w3、x1、x2、x3等等，全部可以作为常量看待。

### 梯度下降的一些实践

> 在实现梯度下降的过程中，有一些可用于优化的实践经验

#### 特征缩放

在房价预测的应用中，假设我们有两个输入特征，一个是房子的房间数，范围在1-4之间，另一个是房子的尺寸，我们选了一个不靠谱的单位，范围在200000平方厘米(其实就是20平米)到3000000平方厘米(300平米)，而输出结果，房价的范围在100万到1000万之间。
x1和x2大小悬殊，这对梯度下降会有什么影响？

{% mathjax %}f_{w_1,w_2,b}(x_1, x_2) = w_1x_1 + w_2x_2 + b{% endmathjax %}
{% mathjax %}w_{1} = w_{1} - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_1{% endmathjax %}
{% mathjax %}w_{2} = w_{2} - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_2{% endmathjax %}

从上面的公式，我们可以依次推导出以下结论：
1. x2很大，多倍于y，所以w2很可能会很小。同理w1可能会很大。
2. alpha固定，x1很小，w1每一步的更新幅度会比较小。而w1又可能比较大，则有可能需要很多次更新，才能让w1更新到合适的值。所以对于w1而言，应该选用比较大的alpha，来减少迭代次数，提高效率。
3. 相反，alpha固定，x2很大，w2每一步的更新幅度会比较大。而w2又可能比较小，运气好的话，可能在很少的更新次数里，w2就可以得到一个比较合适的参数。
但也很有可能，w2更新过头了，比如说w2目前是1，比较合适的参数是2，但是经过一步梯度计算后，变成了11，用力过猛。然后下一步梯度计算又变成了-11，离正确的结果越来越远(这也称为不收敛)。所以，对于w2而言，应该选择更小的alpha，来减小每一步的更新力度。

综上，我们需要根据输入特征的大小范围，给不同的特征选择不同的学习率alpha，才可以让模型高效学习且可以收敛。但还有更简单通用的做法，就是把所有特征，通过缩放，归一到比较接近的大小范围内。

特征缩放有以下几种常见方式：
1. 均值归一化(mean normalization): {% mathjax %}x_1 = \frac{x_1 - \mu_1}{max_1 - min_1}$。其中$\\\mu_1{% endmathjax %}为该输入特征的平均数，max1为其最大值，min1为其最小值
2. Z-score归一化(Z-score normalization):{% mathjax %}x_1 = \frac{x_1 - \mu_1}{\sigma_1}$。其中$\\\sigma_1{% endmathjax %}为该输入特征的`标准差`(standard deviation)。(标准差具体定义可以看百度，python的numpy可以直接进行运算)

#### 是否收敛的判断

![](http://cdn.jsblog.site/16657213593616.jpg)

有两种方式可以判断是否模型收敛:
1. 绘制图形，通过图形可视化判断，随着迭代次数的增加，成本函数的下降是否逐渐趋于平缓，最终基本平行于水平轴。若是则可判断为收敛
2. 定义{% mathjax %}\varepsilon = 10^{-3} = 0.001{% endmathjax %}，当在一次迭代里，成本函数的下降小于这个数，则自动判定为模型收敛。

实际操作中，课程老师更推荐用第一种方式

#### 特征工程 和 多项式回归

特征工程(feature engineering)指的是对现有的特征进行选择和处理，以得到更符合模型的输入特征。
举个例子，还是预测房价，不过这次我们预测的是独栋别墅的价格。我们拿到两个输入特征，x1是房子有多少层楼，x2是每层楼的尺寸。
我们通过对特征进行组合，可以得到一个新的特征{% mathjax %}x_3=x_1x_2{% endmathjax %}，其现实意义就是x3为房子的总尺寸。

除了对多个输入特征进行组合，我们还可以选择对单个输入特征进行处理，比如乘方。
比如可以设定这样的模型:{% mathjax %}f_{\overrightarrow{W},b}(x) = w_1x + w_2x^2 + w_3x^3 + b{% endmathjax %}

## 逻辑回归(logistic regression)
> 很多场景比如垃圾邮件识别、癌症判断等，不太适合用线性回归，可以使用逻辑回归。
> 逻辑回归用于二元分类(binary classification)，二元分类指的是，输出的结果仅有两种可能的分类
> 需要注意，逻辑回归虽然名称里包含了`回归`，但其其实属于分类算法，之所以叫这个名字主要是历史原因

### 为什么线性回归不适合做分类

![](http://cdn.jsblog.site/16658404134097.jpg)
![](http://cdn.jsblog.site/16658403860358.jpg)
如上图，这是一个肿瘤是否恶性的训练数据，横坐标是肿瘤的大小，纵坐标为1代表肿瘤为恶性，为0代表为良性。使用线性回归算法训练出一条拟合数据的直线，我们设定在直线上纵坐标大于0.5的点，肿瘤是恶性的，小于0.5则是良性的。
可以看到图一里，蓝色的直线完全拟合训练数据，所有训练数据的判断都是正确的。而当增加了一个特殊的训练数据，肿瘤很大，而且其为恶性的时候，训练出来的直线为图二的绿线。可以发现，因为增加了这么一个训练数据，导致有些原先判定为恶性肿瘤的数据，在新的模型里被判定为良性了。
这显然不符合我们的预期。

### 模型

![](http://cdn.jsblog.site/16658413178106.jpg)
![](http://cdn.jsblog.site/16658420859144.jpg)

通过图形可以看到，我们希望输出的值在0到1之间，而且随着横坐标增加，纵坐标等于1的概率增加。图二的sigmoid函数(又称为逻辑函数)的图形很好地符合要求
其公式为：{% mathjax %}g(z)=\frac{1}{1+e^{-z}}{% endmathjax %}
我们使{% mathjax %}z=\overrightarrow{W}\bullet\overrightarrow{X}+b$，则可以得到完整的逻辑回归的模型：$\\f_{\overrightarrow{W}, b}(\overrightarrow{X})=\frac{1}{1+e^{-(\overrightarrow{W}\bullet\overrightarrow{X}+b)}}{% endmathjax %}

若对于某个特征输入{% mathjax %}f_{\overrightarrow{W}, b}(\overrightarrow{X})=0.7$，则输出y为1的概率为70%，为0的概率为30%。即$\\f_{\overrightarrow{W}, b}(\overrightarrow{X}) = P(y=1)$，而且$\\P(y=1) + P(y=0) = 1{% endmathjax %}

### 决策边界(decision boundary)

假设我们定g(z)>=0.5时，y=1，g(z)<0.5时，y=0(0.5这个阈值根据具体应用而定)。根据sigmoid的图形我们可以看到，当z>=0时，g(z)>=0.5，当z<0时，g(z)<0.5。
也就是这里存在一条边界{% mathjax %}\overrightarrow{W}\bullet\overrightarrow{X}+b=0{% endmathjax %}(也不一定是0，记住前面推导的过程，若阈值不为0.5时，这里也就不为0)。这条边界就称为决策边界。
![](http://cdn.jsblog.site/16658432100109.jpg)
![](http://cdn.jsblog.site/16658432284270.jpg)

通过图形可以知道，决策边界在图形上就是一条划分两个分类的边界线，可能是直线也可能是曲线。

### 成本函数

在逻辑回归中，若依然使用`平方误差成本函数`作为成本函数，则会存在多个局部最小值，导致模型无法收敛，如下图所示:
![](http://cdn.jsblog.site/16658434767594.jpg)

逻辑回归的成本函数公式为：{% mathjax %}J(\overrightarrow{W},b)=\frac{1}{m}\sum_{i=1}^{m}[-y^{(i)}log(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}))-(1-y^{(i)})log(1-f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)})]{% endmathjax %}

公式很复杂，其在来源于最大似然估计(maximum likelihood estimation)，在课程里并没有介绍其选择的理由，只是简单推导了下，这个函数能起到预期的作用。当预测的y值与实际的y值不符合时，成本函数会得到很大的值，对模型起到惩罚的作用，而且很重要的是

### 梯度下降实现

逻辑回归的梯度下降实现，公式和多元线性回归的看起来一摸一样，这一点我其实不太能理解。但是因为高数一时还没捡起来，没办法自己对公式进行求导计算，只能先把这个结论记下来，后面再花时间把这个坑补上，立个flag。

这里直接重新记录下，梯度下降的实现公式，虽然公式看起来和线性回归的一摸一样，但是因为模型的f函数有区别，所以实际计算出来还是有差别的。其他前面提到的，梯度下降的推荐实践比如特征缩放，依然还是适用的。

{% mathjax %}w_{1} = w_{1} - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_1{% endmathjax %}
{% mathjax %}w_{n} = w_{n} - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_n{% endmathjax %}
{% mathjax %}b = b - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)}){% endmathjax %}

## 过拟合(overfitting)问题

过拟合问题指的是模型极度匹配训练数据，但是模型本身并不准确，在预测新的数据时无法得到准确的数据。这就好像，小明没有认真学习，只是把往年的真题试卷全部背了下来，这样在模拟考的时候可以得到很好的分数，但是实际高考时，拿到不认识的题目，会拿到很差的分数。
![](http://cdn.jsblog.site/16658448367505.jpg)
上图左边存在欠拟合问题，右边存在过拟合问题。

解决过拟合问题有三种方式：
1. 收集更多的训练数据。就比如只要给小明足够多的题目，即使只是在纯背题，最后还是有可能在高考时拿到高分的。
2. 当输入特征有很多时，只选择关键的、合适的输入特征。比如给小明买了几百本教辅书籍，反而可能会让小明错误地学习，还不如选几本关键的教辅书给他学。
3. 减少参数的大小，这被称为正则化(regularization)。几百本教辅书已经买好了，不给小明觉得有点浪费，而且还怕万一哪一本就压到了题，干脆还是全部给他学，但是在学习的过程中，自动调低一些不相关的教辅书的权重。里面如果混了一本完全没用的书，那就给他参数权重调到基本为0，不学它。

### 具有正则化的成本函数

直接上结论，公式为: {% mathjax %}J(\overrightarrow{W},b) = \frac{1}{2m}\sum_{i=1}^{m}(f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}w^2_{j}{% endmathjax %}
其中{% mathjax %}\lambda$为正则化参数，可以看到，如果正则化参数很大，则为了最小化成本函数，所有w都趋近于0。而如果正则化参数很小，则对成本函数的影响基本为0，正则化毫无作用。所以为了让正则化能生效，$\\\lambda{% endmathjax %}不能过大也不能过小。

根据公式，我们可以从直觉上判断，为什么正则化能产生作用。正则化其实就是希望通过成本函数，让w默认就有变小的趋势，从而减少某些输入特征对输出结果的影响。

### 具有正则化的梯度下降实现

还是直接上公式，比较容易能够求导出来：
{% mathjax %}w_{j} = w_{j} - \alpha[\frac{1}{m}\sum_{i=1}^{m}[f_{\overrightarrow{W},b}(\overrightarrow{X}^{(i)}) - y^{(i)})x^{(i)}_j] + \frac{\lambda}{m}w_j]{% endmathjax %}
{% mathjax %}b = b - \alpha\frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)}) - y^{(i)}){% endmathjax %}

## 总结

在写这篇学习笔记的时候，我已经同步在学习下一个课程了，我发现其实这节课学习的东西，在真正的机器学习开发的过程中，很可能基本不会使用到，不过我觉得这节课还是很有意义的。

首先课程足够简单，带着我很好的入了门，还上手接触了相应的代码。其次这种基础性的知识，为我后面理解更高级的内容，可能有不小的帮助。

不过学习这个课程还是有几点障碍：
1. 纯英语，只能依赖第三方翻译，虽然随着学习时间的增加，阅读英语的速度也提高了，但还是比中文的课程慢了不少，有些生僻的词还是要靠翻译。不过人工智能很多内容都是英语的，所以还是要强迫自己学英语。
2. 大学学的数学很多忘了，但又有好奇心，所以很多地方自己都会尝试去推导，导致花了不少时间。有时间还是得复习下数学知识，后面可能有用。
3. 有些地方没有深入讲为什么做这个选择，只能先暂时记结论。应该是为了保持课程的简洁，这个可以理解，不过最好还是可以有相关的外部关联知识，可以去查看。这个也还好，后面有兴趣再去逐个了解，目前这个阶段，还是希望能尽快入门。



