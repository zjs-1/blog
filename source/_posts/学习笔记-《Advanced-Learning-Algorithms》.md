---
title: 2 - 《Advanced Learning Algorithms》
date: 2022-11-10 09:31:32
tags: [学习笔记,机器学习]
categories: 技术
toc: true
---

## 神经网络(Neural Networks)

### 背景

神经网络，又称为深度学习算法(deep learning algorithms)。
早期的神经网络试图通过软件来模仿大脑的神经元，但今天的神经网络几乎与大脑的学习方式无关，应该说，直至今天我们还没办法完全知道大脑神经元是怎么样工作的。

![](http://cdn.jsblog.site/16663388169237.jpg)
上图是一个简化的生物神经元模型，一个神经元细胞通过`树突`接受电信号输入，通过`轴突`输出电信号给其他神经元细胞。简化出来，就是一个神经元细胞接受输入，并对外输出，多个神经元细胞互相组合成一个神经网络，人工神经网络就是对这个模型进行模拟。

![](http://cdn.jsblog.site/16663391108025.jpg)
上图解释了为什么我们需要神经网络，随着过去十几年的互联网的发展，可用于训练AI算法的数据量越来越大。但是对于传统的AI算法比如线性回归和逻辑回归而言，越来越多的数据并不能带来显著的性能提升，而越大型的神经网络可以更好地利用数据来提升性能。

### 简单理解神经网络的工作

假设我们有一个场景，通过衣服的价格、运输费、营销、用料等特征，预测某件衣服能够畅销。

我们可以构建这样一个神经网络，如下图：
![](http://cdn.jsblog.site/16664112776081.jpg)
图中，最左侧拥有四个输入，对应衣服的四个输入特征。中间有三个值，分别是衣服的价格是否合适(可负担)、衣服是否知名、衣服的质量。这三个值分别通过前面四个输入，经过算法计算得出，这些算法可以是上节课学过的线性回归或逻辑回归的一种。
最后通过AI算法，对这三个值进行计算，可以得出一个输出值，这个输出值为这件衣服成为畅销品的概率。

可以看出，人工神经网络其实是将多个传统AI算法结合到一起，组合成一个更复杂的AI算法，以完成复杂的任务。越复杂的神经网络，拥有的节点、层数也会更多。


![](http://cdn.jsblog.site/16664118259677.jpg)
再举一个图像识别的例子：
1. 输入是图片的每一个像素点
2. 神经网络第一层的作用是识别各种基础图形特征，比如横线、竖线、斜线等等，每一个神经元负责寻找一个特定的特征
3. 第二层的作用，是把第一层的基础图形特征进行组合，找到更复杂的特征，比如鼻子、眼睛等等
4. 第三层的作用，是把第二层的特征进行组合，组合成更大的特征比如粗糙的脸型
5. 最后一层是输出层，用于确定图片里的人物可能是谁

### 神经网络模型

![](http://cdn.jsblog.site/16664125054741.jpg)

上图是一个神经网络模型，这里有几个名词:
1. 最左边为输入，称为输入层(input layer)或第0层(Layer 0)
2. 中间有三层，统称为隐藏层(hidden layers)
3. 最右侧为输出，称为输出层(output layer)，也是这个模型的第四层
4. 每一层里的小圆圈，代表一个神经元

这样一个模型我们称为四层的神经网络，这四层分别是隐藏层+输出层，不包括输入层。

![](http://cdn.jsblog.site/16664129054551.jpg)
我们将第三层放大，可以看到，每一个神经元都有自己的参数，分别记为{% mathjax %}(\overrightarrow{w}_1^{[3]},b_1^{[3]})$,$\\(\overrightarrow{w}_2^{[3]},b_2^{[3]})$,$\\(\overrightarrow{w}_3^{[3]},b_3^{[3]})$，这三个神经元的输入均为$\\\overrightarrow{a}^{[2]}$，其为第二层的输出，是一个长度为5(取决于第二层的神经元数量)的向量，因此$\\\overrightarrow{w}_1^{[3]}{% endmathjax %}等也是一个长度为5的向量。

通过sigmoid函数，这三个神经元分别计算出{% mathjax %}a_1^{[3]}$,$\\a_2^{[3]}$,$\\a_3^{[3]}{% endmathjax %}，这三个数字组成一个向量，作为下一层的输入。
以上代数，右上角的`[l]`，代表其属于第l层，右下角`j`，代表其属于所在层的第j个神经元。所以我们可以得出模型公式：{% mathjax %}a^{[l]}_j=g(\overrightarrow{w}_j^{[l]}\bullet\overrightarrow{a}^{[l-1]}+b_j^{[l]}){% endmathjax %}
要注意的是，这里的`g()`为激活函数，后面会讲到

### 在tensorflow中实现神经网络

![](http://cdn.jsblog.site/16664232367112.jpg)
回顾之前逻辑回归算法的训练步骤，总共有三步:
1. 定义模型及其实现
2. 指定loss函数和成本函数
3. 通过数据进行训练，使得成本函数达到最低

通过tensorflow实现神经网络，过程也很类似：

```python
# 引入tensorflow类库
import tensorflow as tf
# 用于组合多个层，自动实现数据向前传播
from tensorflow.keras import Sequential
# Dense是神经网络层的一种类型，目前只用到Dense这种类型，后面还会用到其他
from tensorflow.keras.layers import Dense
# 二值交叉熵，即上节课学的逻辑回归的loss函数
from tensortflow.keras.losses import BinaryCrossEntropy

# units为每一层所拥有的神经元数量
# 激活函数(activation function)为该层神经元使用的算法，此处用的是sigmoid函数，最常用的relu函数，后面会讲，此外还有linear函数
model = Sequential(
    Dense(units=25, activation='sigmoid'),
    Dense(units=15, activation='sigmoid'),
    Dense(units=1, activation='sigmoid'),
)
# loss用于指定loss函数，除了二值交叉熵，还可以用MeanSquaredError(平方误差成本)
model.compile(loss=BinaryCrossEntropy())
# fit的过程即为用数据训练模型，最小化成本函数的过程，
model.fit(X, Y, epochs=100)
# predict即用训练好的模型，预测结果
model.predict(x_new)
```

### 激活函数的选择
![](http://cdn.jsblog.site/16664246134917.jpg)

上图是画了三种常见激活函数的可视化图形，其对应的公式分别为:
sigmoid: {% mathjax %}g(z)=\frac{1}{1+e^{-z}}{% endmathjax %}
linear: {% mathjax %}g(z)=z{% endmathjax %}
relu: {% mathjax %}g(z)=max(0, z){% endmathjax %}

在输出层中，激活函数的选择可以根据输出值进行选择:

* 若输出值为0或1，即二元分类问题，则可以选择sigmoid作为输出层的激活函数。
* 若输出为任意正负数，则选择linear作为激活函数。
* 若输出为0或任意正数，则选择relu

![](http://cdn.jsblog.site/16664260180745.jpg)

对于隐藏层而言，相较于sigmoid函数，更常用的选择是relu。老师给出的理由是，因为sigmoid函数两头趋于平缓，因此最终计算成本函数时，会有多处平缓，导致学习速度比较慢，而relu只有一侧平缓，学习速度较快。(这个理由我无法完全理解，立个flag，后面通过代码实际绘个图看下)

而另一个激活函数，linear则不会在隐藏层中使用，通过下图可以看出原因。两层神经元都使用linear作为激活函数时，最终输出的结果还是一个线性回归，也就是这样一个多层的神经网络，所能完成的任务，跟传统的线性回归没有任何本质区别，多此一举。
![](http://cdn.jsblog.site/16664265009094.jpg)

## Softmax回归

### 背景

先回顾下前面学的几个关键内容:
1. 线性回归，用于在无限的结果里预测一个结果，比如房价预估
2. 然后是逻辑回归，用于二元分类场景，比如邮件是否为垃圾邮件，某个动物是猫还是狗
3. 这节课先学了神经网络，可以看出神经网络与前面两个算法并不是相对立的，可以理解为它是对算法的加强，通过大量的神经元来让模型具有解决复杂问题的能力。神经网络的输出层选择什么样的激活函数，决定了其可用于完成什么样的任务

这节课学的Softmax回归，是逻辑回归的推广。逻辑回归用于解决二元分类问题，而Softmax回归则可以解决多类分类(Multiclass Classification)问题，比如区分某个动物是狗、是马、是猫还是人等等，还有0到9的数字识别

### 模型

假设分类的结果y=1,2,3,...,N
则：
{% mathjax %}z_j=\overrightarrow{W_j}\bullet\overrightarrow{X}+b_j{% endmathjax %} 其中{% mathjax %}j=1,...,N{% endmathjax %}

{% mathjax %}a_j=\frac{e^{z_j}}{\sum_{k=1}^{N} e^{z_k}}=P(y=j|\overrightarrow{X}){% endmathjax %}

其中{% mathjax %}a_j{% endmathjax %}即为该输入，被分类为j的概率有多大

### 成本函数

{% mathjax %}
loss(a_1,...,a_N,y)=\begin{cases}
-log(a_1) & \text{ if } y=1 \\ 
-log(a_2) & \text{ if } y=2 \\ 
& \vdots \\
-log(a_N) & \text{ if } y=N
\end{cases}$
{% endmathjax %}

{% mathjax %}J(\overrightarrow{W},b)=\text{average loss}{% endmathjax %}

> 课程基本没有对模型和成本函数进行推导，公式是通过逻辑回归的公式类比过来的，直接理解即可

### 具有Softmax输出的神经网络

前面学习的神经网络，输出层只有一个神经元，用于区分多个分类比较困难。所以在多类分类问题中，我们有多少个分类，在输出层就设置多少个神经元，每一个神经元的输出分别就是前面提到的{% mathjax %}a_1,a_2,...,a_n{% endmathjax %}，如下图：
![](http://cdn.jsblog.site/16675459355678.jpg)

其对应的tensorflow实现如下：
![](http://cdn.jsblog.site/16675460015649.jpg)

可以看到上面图里蓝色字写着，这种实现方式并不推荐，主要是因为程序的计算可能存在精度问题，中间步骤越多，因为精度导致的误差就会越大。

通过以下的python代码可以理解上面这句话:

```python
x1 = 2.0 / 10000
print(f"{x1:.18f}")
x2 = 1 + (1/10000) - (1 - 1/10000)
print(f"{x2:.18f}")
```

所以上面tensorflow的实现推荐换成下图:
![](http://cdn.jsblog.site/16675481375456.jpg)

其本质就是，在神经网络输出结果时，直接使用线性回归，不做对数运算，在最后计算误差成本再进行对数运算，减少中间结果的输出，以减少精度问题导致的误差。

### 多标签分类(Multi-label Classfication)问题

多类分类问题，典型的场景是，区分某个动物是什么。还有另一类很常见的场景，我们要给某张图片打上多个标签，比如在汽车自动驾驶的场景中，算法要对摄像头拍摄的图片进行判断，判断图片里是否有人、是否有汽车、是否有自行车等等，以便于后续作出不同的动作。

这一场景中，最简单粗暴的方式，就是使用多个神经网络，将图片作为输入，分别输出是否有人、是否有汽车、是否有自行车。

其实还有另一种更合适的做法，就是用一个神经网络，但是存在多个输出神经元，每个神经元都用sigmoid作为激活函数，分别输出是否有人、有汽车、有自行车，如下图。
![](http://cdn.jsblog.site/16678019221963.jpg)

## 一些扩展知识

### Adam算法

回想之前学的，在最小化成本函数的时候，我们一直使用的是梯度下降函数。在梯度下降函数中，{% mathjax %}\alpha$是一直固定不变的，所以$\\\alpha{% endmathjax %}的选择就很重要，当其过大时，成本函数会来回横跳，难以收敛，当其过小时，效率又太低。

Adam算法在这一方面进行了优化，当学习率过低时，会将其调大，当学习率过高时又会自动将其降低。不过在这个课程中，并没有深入去将Adam算法是如何实现这一优化的。

下面的代码是在tensorflow中对Adam算法的使用:

```python
model = Sequential([
    tf.keras.layers.Dense(units=25, activation='sigmoid'),
    tf.keras.layers.Dense(units=10, activation='linear'),
])
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)
model.fit(X,Y,epochs=100)
```

### 卷积层

在前面的神经网络的例子中，我们一直使用Dense层，也称为全连接层。它的特点是，前一层的所有输出，都会作为这一层所有神经元的输入。

还有另外一种常见的层类型，卷积层(Convolutional Layer)，它的特点是，每一个神经元，只会使用上一层部分神经元的输出。它的优势在于:
1. 提高计算速度
2. 减少训练数据，可以降低过拟合的概率

下图是一个卷积神经网络(当神经网络里存在多个卷积层时，有时我们也将该神经网络叫为卷积神经网络)的例子，这个神经网络将一个心电图作为输入，切分成100片，第1-20片作为第一个神经元的输入，第11-30片作为第二个神经元的输入，依次类推。

![](http://cdn.jsblog.site/16678054497230.jpg)

除了全连接层和卷积层之外，还有一些其他神经网络层，课程没有谈及，后面可以自行了解。

## 应用建议

> 以下是课程里，一些机器学习中的实践建议

### 通过测试数据集评估模型

将数据集拆分成独立的两份，分别用于模型的训练和测试。训练数据集和测试数据集的比例没有明确要求，不过很多时候会用7:3的比例。

使用训练数据集，对模型训练完毕后，使用测试数据集计算{% mathjax %}J_{test}(\overrightarrow{W},b)$，计算出来的结果可用于衡量模型的准确度。需要注意的是，在计算$\\J_{test}{% endmathjax %}时，不需要包含参数正则化的部分，如下图：
![](http://cdn.jsblog.site/16678064920625.jpg)

对于分类问题，还有另一种更常用的方式，就是计算测试数据集里，错误分类的数量，如下图：
![](http://cdn.jsblog.site/16678066601827.jpg)

### 通过交叉验证集选择模型

当特征过少，模型可能存在欠拟合问题，当特征过多，又会可能出现过拟合问题。通过交叉验证集可以让我们选择合适的模型。

将数据集按一定比例分为独立的三部分，分别是训练数据集、交叉验证集(cross validation)和测试集。通过训练数据集对多个模型进行训练，使用交叉数据集计算成本{% mathjax %}J_{cv}$，该指标用于衡量哪一个模型更适合，最后再用测试数据集计算成本$\\J_{test}{% endmathjax %}，用于衡量模型的准确度。

这里可能会有点疑惑，为什么不直接用测试来选择模型，而要专门区分一个交叉验证集。如果我们用测试数据集来选择成本最低的模型，相当于我们对测试结果做了一次人为筛选，最后在衡量模型准确度的时候，就不是客观准确的了。
一个不太贴切的例子便于理解，某厂生产了10万支手机，国家要求在指定的测试商处进行检测，良品率要达到90%才能上市，于是工厂找了10个测试商各寄出100支测试机进行测试，得到的良品率分别是50%、55%、60%、65%、70%、75%、80%、85%、90%、92%，然后工厂宣称其拿到了92%的良品率，要求上市，这合理吗？肯定不合理，厂家至少要再多寄一批完全独立的测试机，给最后的测试商进行测试，才能保证结果的客观准确。

### 偏差(bias)和方差(variance)

![](http://cdn.jsblog.site/16678876995235.jpg)

上面这幅图在课程一里我们看到过相似的，其中三个模型的特征如下

* 左边的模型我们称其存在`欠拟合`问题，其存在`高偏差`，体现在数据上就是其{% mathjax %}J_{train}$和$\\J_{cv}{% endmathjax %}都比较高。
* 右边的模型，我们称其存在`过拟合`问题，其存在`高方差`，体现在数据上就是其{% mathjax %}J_{train}$低但是$\\J_{cv}{% endmathjax %}高。
* 中间的模型则比较符合实际情况，其{% mathjax %}J_{train}$和$\\J_{cv}{% endmathjax %}都比较低。

简单理解，偏差指的是模型跟实际数据的误差，而方差指的是不同训练数据得到的结果之间的误差。偏差大，说明模型偏离实际数据，模型存在欠拟合问题。方差大，有可能是训练数据得到的误差小，但是用交叉验证数据得到的误差大，说明模型可能存在过拟合问题。

![](http://cdn.jsblog.site/16678883313478.jpg)
通过上图可以看出，当特征的多项式次数太小，模型会存在高偏差，当特征的多项式次数过大则可能会出现高方差。

![](http://cdn.jsblog.site/16678884975720.jpg)
![](http://cdn.jsblog.site/16678886000848.jpg)
参数正则化的系数对模型也会产生影响，从上面可以看出，系数过大，会存在高偏差，系数过小又可能会出现高方差。

看起来我们可以通过{% mathjax %}J_{train}、J_{cv}$来判断模型可能存在什么问题，但这里还存在一个关键问题，我们说$\\J_{train}{% endmathjax %}多高算高、多低算低。课程里给出的建议是：
1. 跟人类的表现水平做对比
2. 和其他算法的表现做对比
3. 根据经验进行猜测

前面我们看了`特征多项式次数`和`正则化系数`对模型的影响，最后再看下`训练数据数量`对模型的影响：
![](http://cdn.jsblog.site/16678900254584.jpg)
![](http://cdn.jsblog.site/16678900102382.jpg)
可以看到，当模型出现高偏差时，训练数据的增加并不能让模型的表现变好，而当存在高方差时，训练数据的增加可以让那个模型表现变好。

总结一下:
当模型存在高偏差的问题时，我们可以考虑增加特征数量、增加特征的多项式次数、降低正则化系数。而当模型存在高方差问题时，我们需要考虑尝试使用更少的特征、获取更多的训练数据、提高正则化系数。

在神经网络中，越大的神经网络，偏差会越低，但更可能出现高方差的问题。不过只要神经网络的正则化系数设置得当，大型的神经网络也可以避免高方差的问题。
所以实践中，一般会使用尽可能大型的神经网络(也要考虑计算资源)并设置合适的正则化系数，以同时减少偏差和方差。

### 机器学习的开发流程

![](http://cdn.jsblog.site/16679707708117.jpg)

![](http://cdn.jsblog.site/16678906530425.jpg)
机器学习的开发，一般是这三个步骤的循环：选择合适的架构->训练模型->分析->重新调整架构。

#### 错误分析

错误分析是很重要的一步，我们可以将模型识别错误的数据，抽样进行查看，通过人工判断出错的原因主要是哪些，选择出错率最高的那些类型进行针对性解决，这样可以最高效地提高表现。

#### 添加数据

有时根据错误分析，我们可能需要补充训练数据，有以下实践建议:
1. 拿到所有类型的数据可能会很慢很昂贵，可以根据错误分析，只获取某些特定类型的数据
2. 对原始图片进行放大、缩小、翻转、扭曲、涂抹，便可以生产更多的图片数据
3. 对原始音频可以插入其他背景音比如闹市声、汽车声等等，可以生产更多的音频数据用于训练
4. 对于一些OCR的应用，还可以利用计算机直接生成图片，进行训练

现阶段，机器学习算法已经发展有一段时间了，现在开发机器学习，更多的工作在于怎么获取数据上而不是研究算法上main了。

#### 迁移学习

假如我们想实现一个神经网络，用于从图片中识别手写的0-9的数字，但我们却没有大量的图片数据可以用于训练。
这时我们可以选择使用别人之前已经训练好的图片识别的神经网络，这个神经网络可能是用于识别猫、识别狗、识别人的，这都无所谓。我们需要替换该神经网络的输出层，以符合我们的实际需求，然后我们可以只训练调整输出层的参数，也可以将该模型已经训练好的参数作为初始参数，对所有参数重新训练。这就是迁移学习。

为什么这种方式可行呢？
前面我们提到，一个图片识别的神经网络，其第一层可能是识别横线、竖线、斜线，第二层是将第一层识别出来的线组合成角，第三层是将第二层的结果组合成曲线或者是特定的形状，最后输出层再将这些形状组合成我们的目标物体，它可以是人、可以是狗、也可以是猫。所以对于图片识别这一任务而言，不管识别的主体内容是什么，前面几层的任务是有共通点的，是可以反复利用的。

迁移学习，需要从网上下载别人已经用海量的数据训练好的神经网络(参数和结构)，该神经网络必须跟你的应用是同种输入类型，比如都是图片或者都是音频，然后使用你应用的特有的数据，对其进行重新训练。

### 倾斜数据集怎么衡量错误

如果我们想通过机器学习算法识别一个比较罕见的疾病，在我们的训练数据集中，只有0.5%的数据是确诊患病的。如果算法针对所有输入，都输出没有患病，则我们的算法正确率高达99.5%，但这能否说明这个算法就是好的算法呢？答案当然是否定的，这样一个算法，完全没办法识别疾病。

因此，针对这种情况，我们必须要计算准确性(Precision)和召回率(recall)。大白话讲一下两者的定义，准确性指的是当我们的算法判断患者患有疾病时，这个结论确实可信的概率是多少；召回率比较难描述，当召回率是0.6时，说明这个算法有40%的概率会漏诊。

他们的计算方式如下图：
![](http://cdn.jsblog.site/16679721901217.jpg)

将准确性和召回率结合，可以得到一个新的指标F1 score，{% mathjax %}F_1score=2\frac{PR}{P+R}{% endmathjax %}，F1 score可以对比不同算法的准确度。

## 决策树

### 模型

![](http://cdn.jsblog.site/16679725575658.jpg)
![](http://cdn.jsblog.site/16679725772213.jpg)

上图是一个用于识别猫的决策树模型，类似于程序员熟悉的二叉树，每个节点判断一个特征，最终叶子节点输出结果。

### 决策树学习

构建决策树有两个关键问题：
1. 当前节点是否还需要继续分裂出子节点
2. 当前节点应该选择什么特征进行分裂

要回答这两个问题，需要先引入一个指标: `熵`，简单来说`熵`可以用来衡量数据的纯度，`熵`越高数据越不纯。其计算公式如下图：
![](http://cdn.jsblog.site/16679760238559.jpg)

在识别猫的案例中，p1是在数据集中猫占所有动物的比例。可以看到，假如6只动物中，6只都是猫时p1=1，6只都不是猫时p1=0，计算出来的H(p1)都为0。而当其中有3只猫时，p1=0.5，H(p1)=1，达到最高。

知道怎么计算`熵`之后，我们先回答第二个问题，应该选择什么特征进行分裂。答案就是在前面所有未使用的特征中，选择一个能让`熵`降低最多的特征。我们将这个指标称为信息增益(Information gain)，其计算公式为:
{% mathjax %}H(p_1^{root})-(w^{left}H(p_1^{left})+w^{right}H(p_1^{right})){% endmathjax %}
{% mathjax %}H(p_1^{root})、H(p_1^{left})、H(p_1^{right})$分别是当前节点的熵、拆分后左子节点的熵和右子节点的熵，$\\w^{left}、w^{right}$分别是拆分后，左右子节点的数据比例，比如左边有5个数据，右边有10个数据，则$\\w^{left}=5/(5+10){% endmathjax %}

然后来回答第一个问题，什么时候停止分裂。有几种判断方式:
1. 当前节点已经都是同一类数据了，比如都是猫了，则没必要再往下分裂
2. 当决策树已经达到了最大深度了
3. 拆分带来的信息增益已经低于预设的门槛了，简单说就是分裂带来的收益很小
4. 当前节点的数据量已经低于预设的门槛了，比如只有两个数据了

回答完上面的两个问题，我们可以看到决策树的学习过程如下:
![](http://cdn.jsblog.site/16679774441303.jpg)

可以看到，使用递归就可以简单地对决策树进行学习。需要注意的是`决策树最大深度`的选择，可以凭感觉预测到，最大深度越大，决策树可以更适应复杂的场景，但同时也会增加过拟合的风险，这和更大的神经网络类似。我们可以通过交叉验证集来选择合适的最大深度。

### 特征的处理

#### one-hot编码

在前面的例子中，每个特征都只有两种数值(是或否)，但实际场景里，有些特征是可能有多个数值的，比如动物毛发的颜色，可以是灰色、黄色、黑色等等。对于这种特征，一种处理方式就是对前面的决策树进行扩展，一个节点可以有多个子节点，不限定为两个。不过这里要介绍的是另一种做法，称为one-hot编码。

还是以动物毛发颜色为例，假设只有灰色、黄色、黑色三种可能(为了简化例子)。那我们可以把这个特征替换成三个: 是否有灰色毛发、是否有黄色毛发、是否有黑色毛发。通过这样的替换，每个特征都只有两种可能的数值。

需要注意，这种操作不仅适用于决策树的学习，还适用于神经网络或其他机器学习算法的训练。

#### 连续数值的特征

并不是所有特征数值都是离散的，现实场景中可能还会有连续数值的特征，比如动物的体重。

对于这种特征，我们可以选择所有相邻两个数值的平均值作为拆分标准，分别计算其`信息增益`，选择最高点的那个数值作为拆分标准。

举个例子，5只动物的体重分别是3.4、3.8、3.9、4.0、4.2，我们分别用3.6、3.85、3.95、4.1作为拆分标准，计算`信息增益`，分别是0.2、0.3、0.35、0.25，则我们选择3.95作为这个特征的拆分标准。

### 决策树用于回归问题

![](http://cdn.jsblog.site/16679787528414.jpg)

如果我们转换场景，不用决策树来判断是否为猫，而是要预测动物的体重，应该要怎么做？
![](http://cdn.jsblog.site/16679816457989.jpg)

首先核心是怎么计算`熵`，在回归问题中，我们选择使用方差来衡量，因为方差衡量的正是一组数据的偏离程度。所以我们只需要把前面的{% mathjax %}H(p_1^{root})、H(p_1^{left})、H(p_1^{right}){% endmathjax %}都换成对应的方差即可，其他学习步骤不需要调整。

![](http://cdn.jsblog.site/16679818370184.jpg)

然后是怎么预测数字？我们将叶子节点的所有数据取平均，作为这个节点的预测数字。

### 决策树 + 集成学习

![](http://cdn.jsblog.site/16679821018529.jpg)

从上图我们可以看到，决策树是不稳定的，训练数据的轻微改动就可能给整颗树带来很大的变化，也就会轻易影响到最终的预测结果。

解决方法是通过训练数据，构建多个不同的决策树，在预测结果时，这些决策树分别给出各自的预测结果，最终选择票数最多的那个结果，这样可以提高整体预测的稳定性。

#### 随机森林算法

##### 放回抽样(sampling with replacement)

一份训练数据，怎么样才能构建多个决策树呢，一种方式是使用`放回抽样`。简单来说，就是从所有训练数据中，随机抽选N次数据，构建一个大小为N的数据集作为新的训练数据。重复构建多次我们就可以获得多个不同的数据集，可以用于训练出不同的决策树。

举个例子，假如我们有红、黄、蓝三个训练数据，我们通过随机抽取，得到红红黄、红蓝蓝、红黄黄、蓝蓝黄、蓝黄黄五个数据集，训练后就可以得到五个决策树。

##### 随机选择特征

使用放回抽样，有时会发现即使训练数据集不同，根节点所使用的拆分特征还是同一个，因此在随机森林算法中还有另一个优化方式，随机选择特征(前提是我们的数据有大量的特征，如果只有两三个就不适用了)。

当在选择当前节点应该选用什么特征进行拆分时，我们不再对所有可用的特征进行比较，而是在这些特征里先随机抽取一部分，再进行比较。比如当前节点可用有100个特征，我们先随机抽取10个特征，然后对他们进行比较，选择`信息增益`最大的那个。(假设总特征是N，一般实践是随机抽取{% mathjax %}\sqrt{N}{% endmathjax %}个特征)

#### XGBoost

同样是抽取数据生成多份训练数据，不同的是倾向于从前一个决策树里预测错误的数据里取数据，以针对性地进行训练决策树。
![](http://cdn.jsblog.site/16679845156772.jpg)

上图是是用开源的工具实现XGBoost算法

## 决策树 vs 神经网络

![](http://cdn.jsblog.site/16679846023986.jpg)

如上图:

* 决策树适用于结构化的数据，非结构化数据不适用。小型的结构树可以人为观测准确率
* 神经网络适用于所有类型的数据，但是训练相对于决策树可能会比较慢，不过可以利用上迁移学习，而且可以连接多个神经网络。


