---
title: 3 - 《Unsupervised Learning, Recommenders, Reinforcement Learning》
date: 2022-11-14 14:23:31
tags: [学习笔记,机器学习]
categories: 技术
toc: true
---

> 前面两节课的内容都是关于监督学习的，这节课主要学习无监督学习的相关内容。

## 聚类算法

### 定义

通过算法在一堆无标签数据里寻找特定的结构特征，将数据划分为两个或多个部分，被划分至同一部分的数据具有某些共同点。

比如在一堆新闻中，将相似的新闻自动识别划分到一起。

### K-means算法

> K-means算法是最常用的聚类算法

#### 简单理解K-means算法

![新建项目](http://cdn.jsblog.site/新建项目.jpg)

上图描绘出了K-means算法运行的过程:

1. 图一，目标是将坐标轴上的所有点，划分为两个集群，要求同一集群内的数据关系紧密。
2. 图二，先在坐标轴上随机选择两个点，分别作为两个集群的初始中心点，用红色、蓝色表示
3. 图三，计算所有点到红、蓝中心点的距离，如果a点离红色中心更近，则标记其属于红色集群，反之亦然。
4. 图四，分别计算蓝色集群和红色集群所有点的中心，将其设置为新的中心点
5. 图五和图六，重复前面两步，计算各点离两个中心点的距离，划分集群；计算集群所有点的中心，调整中心点。
6. 图七，直至所有点不再变换集群，输出结果

用伪代码描述，如下图：
![](http://cdn.jsblog.site/16680640064071.jpg)

#### K-means算法为什么是收敛的

![](http://cdn.jsblog.site/16680667087166.jpg)

K-means算法总结起来就是循环两步: 将点归入最接近的集群、将中心点移到集群的中心。每一步操作都在降低上图的成本函数，直至集群点不再变化，此时成本函数达到最低。所以我们可以发现，K-means算法最终一定是收敛的。

#### 选择初始点

应该能发现，初始中心点的选择最终会影响集群的划分，那我们应该如何选择初始中心点呢？

1. 如果要划分m个集群，则在所有点里，随机选择m个做为初始中心点
2. 运行K-means算法，得到划分后的集群
3. 计算对应的成本函数
4. 循环N次上面1-3步骤(N一般在50-1000之间)，选择成本最低的结果，作为最终的结果

伪代码如下:
![](http://cdn.jsblog.site/16680673539863.jpg)

#### 选择划分的集群数量

有些业务明确知道最终要将数据划分为多少个集群，但有些业务对于划分多少个集群是模棱两可的，这种时候我们应该如何选择集群数量呢？

有一种方法称为肘部法则(elbow method)，随着集群数k的增加，成本函数会逐渐降低，在某个特定的节点，成本函数的降低幅度变小了，我们可选择其作为最佳的集群数量，如下图:

![](http://cdn.jsblog.site/16680674945920.jpg)

需要注意的是，不能以最小化成本函数作为目标，因为集群数量越多，成本函数必然是会降低的，直至集群数量等于所有点的数量，但这样的划分没有实际意义。肘部法则并不适用所有的场景，有些场景下，可能不存在明显的转折点。

还有一些情况，可以根据业务，人为权衡多大的集群数更合适，比如将K-means用于图片压缩，可以根据目标压缩体积来决定集群数。

## 异常检测

### 简单理解

要识别出某个数据是否异常，就是要看这个数据跟其他数据是否有很大的差异。

如下图将所有数据在图上绘制出来，找到这些点的中心，某个位置离这个中心点越远，说明跟其他数据的差异越大，正常数据出现在这个位置的可能性越小。我们可以设定某个标准，比如当可能性低于0.5%时，出现在这里的数据我们认定为异常数据。

![](http://cdn.jsblog.site/16680702941169.jpg)

### 高斯(正态)分布(Gaussion(Normal) distribution)

现实中大量数据的分布都符合高斯分布。在高斯分布里，某个点出现概率可以通过下面的公式计算出来:
{% mathjax %}p(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$ 其中$\\\sigma$为标准差，$\\\sigma^2$为方差，$\\\mu{% endmathjax %}为平均值
参数计算公式如下：
{% mathjax %}\mu=\frac{1}{m}\sum_{i=1}^{m} X^{(i)}{% endmathjax %}
{% mathjax %}\sigma^2=\frac{1}{m}\sum_{i=1}^{m} (X^{(i)} - \mu)^2{% endmathjax %}

![](http://cdn.jsblog.site/16680719492677.jpg)

### 算法实现

![](http://cdn.jsblog.site/16680720143128.jpg)

如上图，异常检测的算法分三步：
1. 选择你觉得合适特征
2. 计算这些特征的平均值和方差
3. 当要检测某个数据时，分别计算各个特征在高斯分布里的概率，将所有概率相乘，得到的结果就是这个数据判定为正常的概率
4. 跟预设的{% mathjax %}\varepsilon{% endmathjax %}进行比较，若概率低于这个值，则将该数据判定为异常

### 评估系统

虽然异常检测属于非监督学习，训练数据都是没有标签化的数据，但是拥有小部分标签化的数据有利于我们评估系统的准确度。
假如我们有10000个正常的数据，和20个异常数据。我们有以下两种方式：
1. 可抽取6000个正常数据作为训练数据集，2000个正常数据加10个异常数据作为交叉验证数据集，2000个正常数据加10个异常数据作为测试数据集。使用训练数据集训练完系统后，通过交叉训练集选择合适的{% mathjax %}\varepsilon{% endmathjax %}，最后使用测试数据集评估系统的准确度。
2. 可抽取6000个正常数据作为训练数据集，4000个正常数据加20个异常数据作为交叉验证数据集，没有测试数据集。使用训练数据集训练完系统后，通过交叉训练集选择合适的{% mathjax %}\varepsilon{% endmathjax %}。

在选择{% mathjax %}\varepsilon{% endmathjax %}和评估系统时，我们可以用上节课提到的准确性(Precision)和召回率(recall)，或者使用F1 score进行衡量。

### 异常检测 vs 监督学习

前面提到，异常检测算法，也是需要一些带标签的数据的(或者默认所有数据都是0，即为正常的)，那我们为什么不使用监督学习来检测异常数据呢？

异常检测适用于正例(异常)很少，反例(正常)很多的数据，而如果正反例数据都很多，则可以使用监督学习。

还有监督学习可以检测过去出现过的正例(异常)，但是对于一些新出现的正例可能没办法很好地识别，而异常检测则可以发现之前未出现过的异常。

所以在疾病检测时，我们希望检测病患是否患某些疾病，这些疾病我们是历史已经发现了的，应该使用监督学习。而在识别金融诈骗时，诈骗手段层出不穷，我们应该使用异常检测，任何与正常转账不同的转账行为都可以被识别出来。

### 特征选择

对于异常检测，我们需要试图确保使用的特征是基本符合正态分布的，如果某些特征并不太符合正态分布，可以尝试进行一些缩放操作，以让其看起来符合。可以使用python的plt.hist方法，将特征绘制出来，肉眼对其进行判断，如下图：
![](http://cdn.jsblog.site/16681324162017.jpg)

当我们发现异常检测系统很难区分正常数据和异常数据时，我们需要进行错误分析。我们可以人为进行判断，那些没办法被识别出来的异常数据，在哪些特征上显得异常，并将其作为异常检测的特征之一。比如在金融诈骗里，我们可以选择用户打字速度或者交易时间等作为特征，这些特征需要人为识别是否可用。

## 推荐系统

### 协同过滤(Collaborative Filtering)

#### 原理

当我们做了一个电影评分网站就像豆瓣，让用户对看过的电影打分，分数在0到5分之间。我们可以根据用户打分推测用户的喜好类型，还可以推测电影属于什么类型，将对应类型的电影推荐给用户。

#### 假设我们知道电影的特征

![](http://cdn.jsblog.site/16683935984639.jpg)
如上图，假如我们知道每一部电影的特征，比如第三部电影，浪漫指数0.99，动作指数为0。根据Alice对其他四部电影的评分，我们可以计算出Alice对电影的喜爱偏好，再用Alice的偏好，结合第三部电影的特征，就可以预测Alice对其可能的评价。可以看到这就是一个线性回归的模型。

确定了模型，接下来就需要确定成本计算，如下图：
![](http://cdn.jsblog.site/16683939481571.jpg)
![](http://cdn.jsblog.site/16683939901542.jpg)

#### 假设不知道电影的特征 但 知道用户的偏好

![](http://cdn.jsblog.site/16683943674501.jpg)
如上图，我们已经知道用户的偏好，用w和b表示，但是所有电影的特征还不清楚，我们同样可以推测出电影的特征，这依然是一个线性回归模型。

其成本计算如下图：
![](http://cdn.jsblog.site/16683944634695.jpg)

#### 预测特征和偏好

如果我们即不知道电影的特征，又不知道用户的偏好，我们是否还能预测用户对电影的评分呢？答案是可以的，其实就是前面两种情况的结合，通过已知的评分推测电影特征和偏好，再通过特征和偏好来预测分数，这就是协同过滤算法。它的模型还是一个线性回归模型，成本计算如下图：
![](http://cdn.jsblog.site/16683946584034.jpg)

其与我们第一节课学的线性回归很类似，不过这个模型除了要学习w和b之外，还得学习x，也就是电影的特征，如下图：
![](http://cdn.jsblog.site/16683947717975.jpg)

协同过滤并不特指某一种模型，如果我们的场景是预测用户会不会点赞某一部电影，这时候就适合用逻辑回归模型，其模型和成本函数如下图：
![](http://cdn.jsblog.site/16683950650664.jpg)
![](http://cdn.jsblog.site/16683950934454.jpg)

### 协同过滤的一些细节

#### 均值归一化(Mean normalization)

当一个新用户从未对任何一部电影做过评价，从成本函数我们可以看出，其偏好w和b肯定会被设置为0，因此在预测他对电影的评分时，得到的结果都是0。这明显不是我们想要的，一个新用户我们就不给他推荐电影了吗？

所以我们对模型加以优化，我们计算每一部电影的平均得分，将所有分数减去这个平均分，这些数据用于训练。而模型预测出的结果，需要再加上电影的平均分，才是最终的预测评分。如下图：
![](http://cdn.jsblog.site/16683967886347.jpg)

可以看出，这样操作之后，假如一个用户从未对任何一部电影打过分，算法会以电影的平均分作为其预测评分。

#### 查找相似的电影

![](http://cdn.jsblog.site/16683975360734.jpg)

计算两部电影之间，所有特征的差的平方和，就可以得到两部电影的差异，这个差异越小，说明两部电影越相似。

#### 协同过滤的限制

* 冷启动问题，比如基本没评过分的用户或者基本没被评过分的电影，没办法准确预测
* 即使我们知道一些有用的用户特征或项目特征可以帮助我们预测结果，但是却没办法在协同过滤中使用

### 基于内容的过滤(Content-based Filtering)

#### 原理

协同过滤是根据其他相似用户的打分，来预测用户的打分。基于内容过滤，则是基于用户和电影的特征，来预测打分。

![](http://cdn.jsblog.site/16683987966126.jpg)
![](http://cdn.jsblog.site/16683989267930.jpg)

如上图，我们根据用户特征如年龄、性别、国家、看过的电影等等特征，计算出用户的特征向量。再根据电影的特征如年份、类型、评价等特征，计算出电影的特征向量。然后将这两个向量结合起来，预测出用户对电影的评分。

#### 基于内容过滤的神经网络

![](http://cdn.jsblog.site/16683990780045.jpg)
可以看到，基于内容过滤的神经网络，其实就是两个神经网络的组合，一个是用户特征计算的神经网络，一个是电影特征计算的神经网络，两个神经网络通过点乘的方式结合到一起。

#### 大量数据时怎么高效推荐

假如我们有上百万的电影，要给一个用户推荐一部电影时，需要对所有电影进行计算，效率将会很低。

在推荐电影时，可以分两步走，检索(retrieval)和排序(ranking)：

1. 检索：从所有电影中，取一部分作为候选，选取规则可以如下：
    * 从用户过去看过的十部电影中，各取十部相似的电影
    * 从用户最常看的三类电影中，各取前十部电影
    * 用户所在国家，评分最靠前的二十部电影
    * 将以上选取的电影放到一个列表里，去重并去掉用户已经看过的电影
2. 排序：将上面选取出来的电影，使用训练好的模型预测分数，并按分数进行排序展示。

几个注意点：
1. {% mathjax %}v_m$代表电影的特征，两部电影的$\\v_m{% endmathjax %}差距越小，说明它们越相近。而这个数据，对于影评网站而言，是可以提前计算好的。
2. 检索时，选取更多电影则会更精准，但是效率会更低，所以需要做权衡

## 强化学习

> 这一章没看太懂，后面好好补一下，再来完善笔记


